{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from string import punctuation\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import summa\n",
    "from summa.summarizer import summarize\n",
    "import benepar # requires Tensorflow, although we'll use torch otherwise\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import spacy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3229b59f274377acf7926694efb203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [00:52<00:00, 7.65MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # we'll use GPT2 to generate sentences\n",
    "# load BERT model\n",
    "model_BERT = SentenceTransformer('bert-base-nli-mean-tokens') # we'll use BERT to filter sentences based on similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n",
      "0.3.8\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import sentence_transformers\n",
    "print(transformers.__version__)\n",
    "print(sentence_transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en2 to\n",
      "[nltk_data]     /Users/mark/nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mark/opt/anaconda3/envs/test_qa_venv/lib/python3.6/site-packages/benepar/base_parser.py:197: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mark/opt/anaconda3/envs/test_qa_venv/lib/python3.6/site-packages/benepar/base_parser.py:197: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mark/opt/anaconda3/envs/test_qa_venv/lib/python3.6/site-packages/benepar/base_parser.py:202: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mark/opt/anaconda3/envs/test_qa_venv/lib/python3.6/site-packages/benepar/base_parser.py:202: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "#nltk.load(\"punkt\")\n",
    "benepar.download(\"benepar_en2\")\n",
    "benepar_parser = benepar.Parser(\"benepar_en2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Get our text file and do some preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(\"../sample_texts/amazon.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip any punctuation from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Wrapper function to perform any text cleaning \n",
    "        that we'd want to do\n",
    "    \"\"\"\n",
    "    text = text.strip(punctuation)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean our text\n",
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Let's summarize our text, using the summarizer\n",
    "\n",
    "Here's the implementation, from the `summa` library, that we'll be loading in: \n",
    "https://github.com/summanlp/textrank/blob/master/summa/summarizer.py\n",
    "\n",
    "It is an implementation of the TextRank algorithm, detailed in the following paper: \n",
    "https://www.aclweb.org/anthology/W04-3252.pdf\n",
    "\n",
    "Here's a great summary detailing how the TextRank algorithm works (fun fact - it was inspired by the PageRank algorithm, which inspired the creation of Google by the algorithm's creators, Larry Page and Sergey Brin, cited in this paper: http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf): https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text, ratio = 0.3):\n",
    "    \"\"\"\n",
    "        Get our sentences to use. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # sets up our sentence to be summarized\n",
    "    sentences = summarize(text, ratio = ratio)\n",
    "    \n",
    "    # split by sentence, using sent_tokenize method\n",
    "    sentences_list = tokenize.sent_tokenize(sentences)\n",
    "    \n",
    "    # do some regex cleaning\n",
    "    cleaned_sentences_list = [re.split(r'[:;]+', x)[0] for x in sentences_list]\n",
    "    \n",
    "    return cleaned_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In 2002, the corporation started Amazon Web Services (AWS), which provided data on Web site popularity, Internet traffic patterns and other statistics for marketers and developers.',\n",
       " 'That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through the company internet site.',\n",
       " \"Amazon.com's product lines available at its website include several media (books, DVDs, music CDs, videotapes and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal-care items, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items and toys & games.\",\n",
       " 'Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Delaware.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how this looks\n",
    "cleaned_text = get_sentences(text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the punctuation again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(x) for x in cleaned_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this looks so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2002, the corporation started Amazon Web Services (AWS), which provided data on Web site popularity, Internet traffic patterns and other statistics for marketers and developers\n",
      "\n",
      "\n",
      "That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through the company internet site\n",
      "\n",
      "\n",
      "Amazon.com's product lines available at its website include several media (books, DVDs, music CDs, videotapes and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal-care items, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items and toys & games\n",
      "\n",
      "\n",
      "Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Delaware\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in cleaned_text:\n",
    "    print(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Split up our sentences, using the Berkley Constituency parser\n",
    "\n",
    "We're going to use the Berkley Constituency parser to split a sentence at the ending verb phrase or noun phrase. \n",
    "\n",
    "For example, if the sentence were:\n",
    "\n",
    "`Jeff Bezos was working on Wall Street before he started Amazon, but he moved west because of the opportunities there`, \n",
    "\n",
    "we can take several approaches to generate false sentences based off this, such as changing a verb (e.g., changing \"started\" to \"left\"), changing a noun (e.g., changing \"Amazon\" to \"Microsoft\"), adding negation (e.g., changing \"working on Wall Street\" to \"not working on Wall Street\") or changing a named entity (e.g,. changing \"Jeff Bezos\" to \"Bill Gates\")\n",
    "\n",
    "For our use case, we'll start by just changing a noun phrase or changing a verb phrase. In our particular implementation, we'll split the sentence at the ending verb phrase or noun phrase. \n",
    "\n",
    "If we were to split at the end of the last verb phrase for our example above, we'd get something like:\n",
    "\n",
    "`[\"Jeff Bezos was working on Wall Street before he started Amazon, but he\", \"moved west because of the opportunities there\"]`\n",
    "\n",
    "Now, what we can do is take the first part of our phrase (`\"Jeff Bezos was working on Wall Street before he started Amazon, but he\"`) and we can ask GPT2 to complete the sentence for us. \n",
    "\n",
    "Similarly, if we were to split at the end of the last noun phrase for our example, we'd get something like:\n",
    "\n",
    "`[\"Jeff Bezos was working on Wall Street before he started Amazon, but he moved west because of the \", \"opportunities there\"]`\n",
    "\n",
    "We can take the first part of the phrase (`\"Jeff Bezos was working on Wall Street before he started Amazon, but he moved west because of the \"`) and ask GPT2 to complete the sentence for us. \n",
    "\n",
    "In this way, we can create false sentences using GPT2. We leverage the Berkley Constituency parser because it parses our sentence in a way such that we'll be able to isolate the last verb phrase or last noun phrase (depending on what comes last in the tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened(tree):\n",
    "    \n",
    "    \"\"\"\n",
    "        Flattens the tree structure that we'll get from the Berkley\n",
    "        parser, to allow us to easily work with it\n",
    "    \"\"\"\n",
    "    \n",
    "    final_sentence_str = None\n",
    "    if tree is not None:\n",
    "        sent_str = [\" \".join(x.leaves()) for x in list(tree)]\n",
    "        final_sentence_str = [\" \".join(sent_str)][0]\n",
    "    return final_sentence_str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_portion(main_string, substring):\n",
    "    \n",
    "    \"\"\"\n",
    "        Here, we get a string of our last verbphrase or\n",
    "        nounphrase. \n",
    "    \"\"\"\n",
    "    \n",
    "    combined_substring = substring.replace(\" \", \"\")\n",
    "    \n",
    "    main_string_list = main_string.split()\n",
    "    \n",
    "    last_index = len(main_string_list)\n",
    "    \n",
    "    for i in range(last_index):\n",
    "        \n",
    "        check_string_list = main_string_list[i:]\n",
    "        \n",
    "        check_string = \"\".join(check_string_list)\n",
    "        \n",
    "        if check_string == combined_substring:\n",
    "            return \" \".join(main_string_list[:i])\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rightmost_VP_or_NP(tree, last_NP = None, last_VP = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        Recursive function, to get the rightmost verb phrase (VP) \n",
    "        or noun phrase (NP), which corresponds to the VP or NP \n",
    "        that occurs at the end of the sentence\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # if we don't have more nodes to traverse, we know we've hit the end\n",
    "    if len(tree.leaves()) == 1:\n",
    "        return get_flattened(last_NP), get_flattened(last_VP)\n",
    "    \n",
    "    # get our last subtree\n",
    "    last_subtree = tree[-1]\n",
    "    \n",
    "    # check if we either have NP or VP:\n",
    "    if last_subtree.label() == \"NP\":\n",
    "        last_NP = last_subtree\n",
    "    elif last_subtree.label() == \"VP\":\n",
    "        last_VP = last_subtree\n",
    "        \n",
    "    return get_rightmost_VP_or_NP(last_subtree, last_NP, last_VP)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_completions(all_sentences):\n",
    "    \n",
    "    \"\"\"\n",
    "        Returns a dictionary of our sentences as well \n",
    "        as the same sentences, just without their terminal\n",
    "        VP or NP\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_completion_dict = {}\n",
    "    \n",
    "    # loop through all of our sentences\n",
    "    for individual_sentence in all_sentences:\n",
    "\n",
    "        # parse any additional punctuation\n",
    "        sentence = individual_sentence.strip(r\"?:!.,;\") \n",
    "        \n",
    "        # get parsed tree\n",
    "        tree = benepar_parser.parse(sentence)\n",
    "        \n",
    "        last_NP, last_VP = get_rightmost_VP_or_NP(tree)\n",
    "        \n",
    "        phrases = []\n",
    "        \n",
    "        if last_VP is not None:\n",
    "            VP_string = get_last_portion(sentence, last_VP)\n",
    "            if VP_string is not None:\n",
    "                phrases.append(VP_string)\n",
    "            else:\n",
    "                phrases.append(\"\")\n",
    "        if last_NP is not None:\n",
    "            NP_string = get_last_portion(sentence, last_NP)\n",
    "            if NP_string is not None:\n",
    "                phrases.append(NP_string)\n",
    "            else:\n",
    "                phrases.append(\"\")\n",
    "             \n",
    "        # get our sentence that we want GPT2 to complete\n",
    "        longest_phrase = sorted(phrases, key=len, reverse=True)\n",
    "        \n",
    "        if len(longest_phrase) == 2:\n",
    "            first_sentence_len = len(longest_phrase[0].split())\n",
    "            second_sentence_len = len(longest_phrase[1].split())\n",
    "            \n",
    "            if (first_sentence_len - second_sentence_len) > 4:\n",
    "                del longest_phrase[1]\n",
    "                \n",
    "        if len(longest_phrase) > 0:\n",
    "            sentence_completion_dict[sentence] = longest_phrase\n",
    "            \n",
    "    return sentence_completion_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the functions, let's get our sentences for GPT2 to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_completion_dict = get_sentence_completions(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'In 2002, the corporation started Amazon Web Services (AWS), which provided data on Web site popularity, Internet traffic patterns and other statistics for marketers and developers': ['In 2002, the corporation started Amazon Web Services (AWS), which provided data on Web site popularity, Internet traffic patterns and other statistics for'],\n",
       " 'That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through the company internet site': ['That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through',\n",
       "  'That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies'],\n",
       " \"Amazon.com's product lines available at its website include several media (books, DVDs, music CDs, videotapes and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal-care items, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items and toys & games\": [\"Amazon.com's product lines available at its website include several media (books, DVDs, music CDs, videotapes and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal-care items, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items and\"],\n",
       " 'Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Delaware': ['Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle,']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_completion_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Filter sentences and generate false sentences\n",
    "\n",
    "For our use case, we'll use GPT2 to generate false sentences, and we'll use BERT to determine the similarity of our sentences (since we only want to keep the sentences that are not similar to our original sentence, so that they will be clearly, unequivocally false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_similarity(original_sentence, new_sentences_list, num_vals = 3):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        Sort our GPT-2 generated sentences by how similar they are to our original sentence. \n",
    "        We want to select sentences that are not similar to our original sentence (since these\n",
    "        are going to be the statements that are most clearly false)\n",
    "        \n",
    "        We will use BERT to perform the similarity calculation\n",
    "        \n",
    "        Args:\n",
    "            • original_sentence: our original sentence\n",
    "            • new_sentences_list: our new fake sentences\n",
    "            • num_vals: number of dissimilar sentences that we want to use\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # encode the sentences from GPT2 into BERT's format (each sentence is a 1-D vector with 768 columns)\n",
    "    sentence_embeddings = model_BERT.encode(new_sentences_list)\n",
    "    \n",
    "    # do same for original sentence\n",
    "    original_sentence_list = [original_sentence]\n",
    "    original_sentence_embeddings = model_BERT.encode(original_sentence_list)\n",
    "    \n",
    "    # get number of matches, then loop through and sort by dissimilarity\n",
    "    number_top_matches = len(new_sentences_list)\n",
    "    \n",
    "    dissimilar_sentences = []\n",
    "    \n",
    "    for query, query_embedding in zip(original_sentence_list, original_sentence_embeddings):\n",
    "\n",
    "        # calculate distance between original sentence and false sentences\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "        \n",
    "        # get list of distances + indices, then sort\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x:x[1])\n",
    "        \n",
    "        # get dissimilarity score (our distance so far shows)\n",
    "        # how close they are, so we want embeddings\n",
    "        # that are far away from the sentence embedding\n",
    "        for idx, distance in reversed(results[0:number_top_matches]):\n",
    "            \n",
    "            score = 1 - distance\n",
    "            \n",
    "            if score < 0.9: # arbitrary threshold\n",
    "                dissimilar_sentences.append(new_sentences_list[idx].strip())\n",
    "                \n",
    "    # sort the dissimilar sentences ascending, and get first n (so, lowest scores = furthest away)\n",
    "    sorted_dissimilar_sentences = sorted(dissimilar_sentences, key = len)\n",
    "    \n",
    "    return sorted_dissimilar_sentences[:num_vals]\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(partial_sentence, full_sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "        Generate false sentences, using GPT2, based off partial sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = torch.tensor([tokenizer.encode(partial_sentence)])\n",
    "    \n",
    "    maximum_length = len(partial_sentence.split()) + 80\n",
    "    \n",
    "    # get outputs\n",
    "    sample_outputs = model.generate(input_ids, \n",
    "                                    do_sample = True,\n",
    "                                    max_length = maximum_length, \n",
    "                                    top_p = 0.9, \n",
    "                                    top_k = 50, \n",
    "                                    repitition_penalty = 10.0,\n",
    "                                    num_return_sequences = 10)\n",
    "    generated_sentences = []\n",
    "    \n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        \n",
    "        decoded_sentences = tokenizer.decode(sample_output, skip_special_tokens = True)\n",
    "        decoded_sentences_list = tokenize.sent_tokenize(decoded_sentences)\n",
    "        generated_sentences.append(decoded_sentences_list[0])\n",
    "        \n",
    "    top_3_sentences = sort_by_similarity(full_sentence, generated_sentences)\n",
    "    \n",
    "    return top_3_sentences\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our true sentence: In 2002, the corporation started Amazon Web Services (AWS), which provided data on Web site popularity, Internet traffic patterns and other statistics for marketers and developers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False sentences (created by GPT2):\n",
      "\n",
      "\n",
      "\n",
      "Our true sentence: That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through the company internet site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False sentences (created by GPT2):\n",
      "a) That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies selling their belongings through an online marketplace and was now taking orders for $500.\n",
      "b) That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies for the U.S. Government.\n",
      "c) That same year, the company started Fulfillment by Amazon which managed the inventory of individuals and small companies in three countries: Mexico, South America and Japan.\n",
      "\n",
      "\n",
      "\n",
      "Our true sentence: Amazon.com's product lines available at its website include several media (books, DVDs, music CDs, videotapes and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal-care items, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items and toys & games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False sentences (created by GPT2):\n",
      "\n",
      "\n",
      "\n",
      "Our true sentence: Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Delaware\n",
      "False sentences (created by GPT2):\n",
      "a) Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Ohio, and has been in operation for almost 20 years.\n",
      "b) Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Ga., where about 2,000 orders a week come from across the country.\n",
      "c) Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Pennsylvania, and now stores at Amazon.com and in over 140 countries.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "choice_list = [\"a)\",\"b)\",\"c)\",\"d)\",\"e)\",\"f)\"]\n",
    "for key_sentence in sentence_completion_dict:\n",
    "    \n",
    "    # get our partial sentence\n",
    "    partial_sentences = sentence_completion_dict[key_sentence]\n",
    "    \n",
    "    # start creating false sentences\n",
    "    false_sentences = []\n",
    "    print(f\"Our true sentence: {key_sentence}\")\n",
    "    \n",
    "    # loop through partial sentencesf\n",
    "    for partial_sentence in partial_sentences:\n",
    "        \n",
    "        # create our false sentences\n",
    "        false_sents = generate_sentences(partial_sentence, key_sentence)\n",
    "        \n",
    "        false_sentences.extend(false_sents)\n",
    "        \n",
    "    print(\"False sentences (created by GPT2):\")\n",
    "    \n",
    "    for idx, false_sent in enumerate(false_sentences):\n",
    "        \n",
    "        print(f\"{choice_list[idx]} {false_sent}\")\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_qa_venv",
   "language": "python",
   "name": "test_qa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
